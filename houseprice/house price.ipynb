{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/data-sets/houseprice/train.csv\")\n",
    "test_df    = pd.read_csv(\"/data-sets/houseprice/test.csv\")\n",
    "test_validation = pd.read_csv(\"/data-sets/houseprice/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df.join(test_validation.set_index(\"Id\"),on=\"Id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([train_df,test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- help functions ------------------\n",
    "\n",
    "#convert all str category to integer\n",
    "def category_to_int(category):\n",
    "    #print category\n",
    "    return hash(category)%32+1\n",
    "    \n",
    "def encode_col(col):\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(col)\n",
    "    return encoder.transform(col)\n",
    "    \n",
    "#deal with NaN\n",
    "def fill_by_col(col):\n",
    "    if col.dtype == np.object:\n",
    "        col.fillna(\"EmptyStuff\",inplace=True)\n",
    "    else:\n",
    "        rand = generate_std_err_int(col);\n",
    "        col[np.isnan(col)] = rand\n",
    "      \n",
    "def generate_std_err_int(col):\n",
    "    average   = col.mean()\n",
    "    std       = abs(col.std())\n",
    "    count = col.isnull().sum()\n",
    "    #return np.random.randint(average - std, average + std, size = count)\n",
    "    return np.full([count,1] ,average)\n",
    "\n",
    "def normalize(df):\n",
    "    return (df - df.mean(axis=0)) / df.std(axis=0)\n",
    "    #data = df\n",
    "    #data = [np.log(tt + 1) for tt in data]\n",
    "    #return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for processing data set\n",
    "def encode_data(data_df):\n",
    "    for i in range(data_df.shape[1]):\n",
    "        col = data_df.ix[:,i]\n",
    "        #fill NaN fields\n",
    "        fill_by_col(col)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "all_encode_data = encode_data(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encode_dummies = pd.get_dummies(all_encode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encode_normalize = normalize(all_encode_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = all_encode_normalize.drop(['SalePrice'], axis=1)\n",
    "y = all_encode_normalize['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2335, 295) (584, 295) (2335,) (584,)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape, X_test.shape,y_train.shape,y_test.shape\n",
    "print type(X_train),type(X_test),type(y_train),type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow MNN\n",
    "\n",
    "# === We build the graph here!\n",
    "houseprice_graph = tf.Graph()\n",
    "\n",
    "with houseprice_graph.as_default():\n",
    "\n",
    "    HIDDEN_SIZE = 200\n",
    "    num_features = X_train.shape[1]\n",
    "    \n",
    "    # create the neural network model\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    prev_loss = tf.Variable(0., trainable=False)\n",
    "    \n",
    "    # first layer\n",
    "    input_layer = tf.placeholder(tf.float32, [None, num_features], name='input')\n",
    "    W1 = tf.Variable(tf.random_normal([num_features, HIDDEN_SIZE], stddev=.01), name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=.01), name='b1')\n",
    "    h1_layer = tf.add(tf.matmul(input_layer, W1), b1)\n",
    "    h1_layer = tf.nn.relu(h1_layer)\n",
    "    h1_layer = tf.nn.dropout(h1_layer, keep_prob, name='h1')\n",
    "    \n",
    "    # second layer\n",
    "    W2 = tf.Variable(tf.random_normal([HIDDEN_SIZE, HIDDEN_SIZE], stddev=.01), name='W2')\n",
    "    b2 = tf.Variable(tf.random_normal([HIDDEN_SIZE], stddev=.01), name='b2')\n",
    "    h2_layer = tf.matmul(h1_layer, W2) + b2\n",
    "    h2_layer = tf.nn.relu(h2_layer)\n",
    "    h2_layer = tf.nn.dropout(h2_layer, keep_prob, name='h2')\n",
    "    \n",
    "    # third layer, output layer\n",
    "    W3 = tf.Variable(tf.random_normal([HIDDEN_SIZE, 1], stddev=.01), name='W3')\n",
    "    b3 = tf.Variable(tf.random_normal([1], stddev=.01), name='b3')\n",
    "    output_layer = tf.matmul(h2_layer, W3) + b3\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1], name='y')\n",
    "    \n",
    "    # cost function, optimizer to global minimal\n",
    "    loss = tf.squared_difference(output_layer, y)\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_constant = 0.01  # Choose an appropriate one.\n",
    "    loss = loss + reg_constant * sum(reg_losses)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-6, beta1=.85, beta2=.9).minimize(loss)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(x_data,y_data,batch_size):\n",
    "    idx = np.random.randint(0, len(x_data), batch_size)\n",
    "    train_batches_x = x_data[idx]\n",
    "    train_batches_y = y_data[idx]\n",
    "    return train_batches_x, train_batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = X_train.as_matrix()\n",
    "y_train_array = np.expand_dims(y_train,1)\n",
    "X_test_array = X_test.as_matrix()\n",
    "y_test_array = np.expand_dims(y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Epochs:0--------\n",
      "train error: 0.56986\n",
      "valid error: 0.905286\n",
      "--------Epochs:1000--------\n",
      "train error: 1.3375\n",
      "valid error: 0.899979\n",
      "--------Epochs:2000--------\n",
      "train error: 1.00534\n",
      "valid error: 0.890898\n",
      "--------Epochs:3000--------\n",
      "train error: 0.888888\n",
      "valid error: 0.874918\n",
      "--------Epochs:4000--------\n",
      "train error: 0.942632\n",
      "valid error: 0.850845\n",
      "--------Epochs:5000--------\n",
      "train error: 0.748846\n",
      "valid error: 0.818257\n",
      "--------Epochs:6000--------\n",
      "train error: 0.908676\n",
      "valid error: 0.777842\n",
      "--------Epochs:7000--------\n",
      "train error: 0.68769\n",
      "valid error: 0.731251\n",
      "--------Epochs:8000--------\n",
      "train error: 0.531988\n",
      "valid error: 0.682158\n",
      "--------Epochs:9000--------\n",
      "train error: 0.675371\n",
      "valid error: 0.635396\n",
      "--------Epochs:10000--------\n",
      "train error: 0.320069\n",
      "valid error: 0.595955\n",
      "--------Epochs:11000--------\n",
      "train error: 0.815884\n",
      "valid error: 0.56629\n",
      "--------Epochs:12000--------\n",
      "train error: 0.697454\n",
      "valid error: 0.545016\n",
      "--------Epochs:13000--------\n",
      "train error: 0.785486\n",
      "valid error: 0.530108\n",
      "--------Epochs:14000--------\n",
      "train error: 0.410551\n",
      "valid error: 0.51905\n",
      "--------Epochs:15000--------\n",
      "train error: 0.478592\n",
      "valid error: 0.510317\n",
      "--------Epochs:16000--------\n",
      "train error: 0.530693\n",
      "valid error: 0.502993\n",
      "--------Epochs:17000--------\n",
      "train error: 0.319731\n",
      "valid error: 0.496731\n",
      "--------Epochs:18000--------\n",
      "train error: 0.532507\n",
      "valid error: 0.491339\n",
      "--------Epochs:19000--------\n",
      "train error: 0.582874\n",
      "valid error: 0.486689\n",
      "--------Epochs:20000--------\n",
      "train error: 0.468898\n",
      "valid error: 0.482556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-295-19267bb4ec2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m.75\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We create our sessions\n",
    "sess = tf.Session(graph=houseprice_graph)\n",
    "\n",
    "# Make sure to run the initialization\n",
    "sess.run(init)\n",
    "\n",
    "NUM_EPOCHS = 200000\n",
    "BATCH_SIZE = 100\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "# get the next batch\n",
    "for i in range(NUM_EPOCHS):\n",
    "    x_batch, y_batch = get_next_batch(X_train_array,y_train_array,BATCH_SIZE)\n",
    "    \n",
    "    sess.run(optimizer, feed_dict={input_layer: x_batch, y: y_batch, keep_prob: .75})\n",
    "    \n",
    "    train_loss.append(sess.run(loss, feed_dict={input_layer: x_batch, y: y_batch, keep_prob: .75}))\n",
    "    valid_loss.append(sess.run(loss, feed_dict={input_layer: X_test_array, y: y_test_array, keep_prob: 1.}))\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print \"--------Epochs:{}--------\".format(i)\n",
    "        print \"train error:\", train_loss[i]\n",
    "        print \"valid error:\", valid_loss[i]\n",
    "\n",
    "#print train_error;\n",
    "#print \"validation error:\", sess.run(error, feed_dict={x:test_inputs, y:test_outputs})\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
